{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tien-Cheng/dele-generative-adversarial-networks/blob/main/DELE_CA2_Part_I_Controllable_%26_Conditional_GANS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYWgFv3Lw7Um"
      },
      "source": [
        "# Background\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "My objective for this project is to attempt to create a GAN model that is able to generate realistic images based on information that is provided to it.\n",
        "\n",
        "Such a task generally falls under two categories:\n",
        "\n",
        "1. Conditional Image Generation: Given a class (e.g. a car), generate an image of that class\n",
        "2. Controllable Image Generation: Given some description of features of the image (e.g. The car should be red in color), generate an image with those features\n",
        "\n",
        "In this project, I hope to tackle the first category (Conditional Image Generation), with the view of eventually tackling the second category if time and resources permit.\n",
        "\n",
        "So, what would I consider to be a success? Well, I would preferably hope to be able to reliably generate images conditionally that look real enough, based on\n",
        "\n",
        "- Eye Power ðŸ•µï¸\n",
        "- Metrics like FID (described later in the report)\n",
        "\n",
        "By real enough, I mean that I hope to get results that are close enough to some of the top GAN models out there, with minimal artifacts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff5jtmM4-S6V"
      },
      "source": [
        "# Google Colab Setup\n",
        "\n",
        "Due to the heavy computational requirements for training a GAN, Google Colab is used as a compute platform. The dataset and utilty functions have been stored on a GitHub repository, and so we need to clone the Git repo. In addition, some additional libraries have to be installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1nKt9XhE_lL",
        "outputId": "1507ed6e-79f2-404b-c7b2-d364d9dba314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# github.com:22 SSH-2.0-babeld-e47cd09f\n",
            "chmod: cannot access '/root/.ssh/rsa': No such file or directory\n",
            "Cloning into 'dele-generative-adversarial-networks'...\n",
            "Warning: Permanently added the ED25519 host key for IP address '192.30.255.112' to the list of known hosts.\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 26 (delta 9), reused 20 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (26/26), 7.52 KiB | 7.52 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "/content/dele-generative-adversarial-networks\n"
          ]
        }
      ],
      "source": [
        "! mkdir -p /root/.ssh\n",
        "with open(\"/root/.ssh/id_rsa\", mode=\"w\") as fp: # Repository Deploy Key\n",
        "    fp.write(\"\"\"\n",
        "-----BEGIN RSA PRIVATE KEY-----\n",
        "MIIJKgIBAAKCAgEAt4IzVm1w9r7xKuS+zYuBb6UNB2NFHQBaRbhih7+HBK+yNSbz\n",
        "lGu9P/sWbFarsY68zKCISb8+K+hulP0ay9OdnCLat9z96eOZ0gX6Iqsh6+szfNvm\n",
        "8m1SJeXc7C6UGmyNIpr33TUpf56y28UFa656rIjff1w20SRKjL2rgu8rx+lxiASL\n",
        "+hXiZi2t1PA6oLD3puD9TOwN85Ct5mutmTjBYQKbmk04Sp8jE9DqloJPpkCJHVh6\n",
        "cJ0bzyVCx4njzdoQeWwPtVa67wyHIXDqH1xZBAkAqt2WAx4npLGgTotPSUaFkDLw\n",
        "co6SnpOLx8ZGrpggX1k2Oh7FOH75nZXHKjrfWtX5pbkw8bxYmNLTErq/t19ULBQi\n",
        "dVyv406ARf1rDOUFoMfsOsc1pd/wf66mcZUn3s3ogI6it5zGrpzCpfrlxHgJQ/Uh\n",
        "gvyWA88J6BRVgMA5cUS4gb/OEmuHvdM9CRY6HELAS35tS85zcRQXipYqngx/dgaV\n",
        "GhHHIWZh1bOkbn1dRV3xQau6KxYOyLI/i+eBFJA3jxvDKlRMVfy1DMSn0DffFlFt\n",
        "ApOAqOccYo124vUthsiWJ9qExdCJ36+tAHpFelfyjAygJMWCZhaWprxvY9VG/lcR\n",
        "6vjNtjaUySWx3l4GTadmbSwARK9gl5Xgbp0qojx1FMpsFcnCsz3y8yB7TYECAwEA\n",
        "AQKCAgBVmHSrxqafYVcKg+H/7Cd21QzrukEdkvGIfcXvvcWTyQQdyMprG4oN0ueV\n",
        "pyO00Xh9FhAcHgk439Tcx+Z81ns4vgU5J+qD8zbngQQ4sYxEB9RfVA84WwerR7mx\n",
        "rNRGMwXt80zUMJznuzWATzkFDkCIQ9vEA1ZKXVwso7fhff/04o2jPUOxZg3RTVM8\n",
        "9MTT+Ve6zk04WQ704jJLPUSfKJsCzf2YjpZIMExjTNpvU98lFAsg1gleh9nV2HJ6\n",
        "snXAqgtvJ5l4IzlUkYpibdG2yRN4T16xVGRJlgI1zuiQWmikLDHWnfwL4za+ouHb\n",
        "UD/d5nWLJAioOXwSqx9xgtCAgS92211ydmKWmsdThHdRDN63ncFPNg7I3ZVdRKOK\n",
        "bdAMev6sP3CXnWO7aRO0sGT4wg7tGbnyE53I4RJXck1aZZGSyOvswucCxEQsnbSP\n",
        "Hr78/kc+5+DJX8pbc0NuLAspkUVoSU75Idrv5B+2UQSb1ZXspfp923s0voRw0sJ8\n",
        "ydOg1n173QOwKnAE++tXQrdPZyU2cHkuvg426snCjlpbogfmmlj8cGGb8EOZcdJv\n",
        "I3r+w2V+9bC2Z7O4OJhe2HlwM0N6F+KBnyJHsxbdP08OqZYzVMiDmg5Rfbkwf8W9\n",
        "arkt9+pAWSix0nkp7qNgD+qkjfrtOxIX//mFbIBWhhq1gSby5QKCAQEA63P0DsuC\n",
        "APhI0/GeSFJ8FXYtVFrX8/DJjOH441VEaQQMlAub6KnlhsOo0nS10A5GqV9/EeZ7\n",
        "ss2w22JwIh+Wk6PtxPU7lMEQRy1eV0GUrQdrE8StlLs36zszMswMafnm4yA4ki6g\n",
        "Lr3BR6Ps38TzLba6mctOFt9T8wV6+/YB2PFi3r5tmX0zYNQi6mbTFnlDv/dzaFOG\n",
        "fT823OuOzuwVvgu90651PutVfPrNhUTTykuGyDee5kkn21HQeguoLDWEIfeV+ujH\n",
        "l/AAT7rpNmxtSl0m+iwYsbKDbX28DCGnWXgeMuFgMUblRvKumChbno81JkJIOOwV\n",
        "+DcWryqTLp17VwKCAQEAx4XN5+PGMg9na7vZW6zU8r92RFKsjjhueygtpxdDUjdV\n",
        "MSYPu4mgO9ab62nf9LQJE2JCN6e8zHWEAooaIt83TzCa6SaYbTEnzin2M9gSYtW6\n",
        "MQ429zq49MOdZfwMfRgfnFAnA8KDIfYqqcPcmnQWHWhNGXyS3CccYw+2+gmRHLoM\n",
        "ohcoVZne6VuMqkEzf8SDaR8k9gwVjqxVqpQN8p81PE00a02k+QDwyNsrcnM19plB\n",
        "kntb9FLuqQf+lmDhe0/9fDqcjIEDz4eonLlFaTrFegGybTQcKD+3uyC0k9njUFwJ\n",
        "Y77I3kJiaoDuXXVxWETS3KvaE2rmjXAEcrN5rkfO5wKCAQEAl+41kQputBOCYwjp\n",
        "Ov/Gw86DB4irCuTYGYmDIaZWw3DycOFg1Gw1CJXerRbUbxGXNRnDFBjmvwUNVzMY\n",
        "6lv5vQEtn0cjECTYTSWQV7ugpVpBFPt3ip6YQbjsm52hcQzpmKuk9WcSw7Z8Lq8v\n",
        "XWFoDZp4pF7U39tx/0INDuK6ZHO2ecblUALDEXsxoJGDKmBLgGa7WJl1EgKlcz6o\n",
        "4wriKMTI0/wh+dy/SCtKTPGRvFqp+S4y4aRZDKOpY+d7uDM8NPLfG43zpS4f9VLF\n",
        "w/GJQFAFo66qrJdlSVS18BoTM59X1Tsq6AE4V2SnltWL8S+1ex+QHPLyZj2d7KAL\n",
        "YywJdwKCAQEApWUG3j6T0nWwfr82nGc2E5ChgluiTTb8Zr1Ustl25hWWWmq5yfV5\n",
        "TYFGuSyICTqg91+Rkr9Ko5aa+tvudI/jMpMRJ0rmOkXwQFfKjwmDnEid0wJ8kA8u\n",
        "uT/bH2qEE8LGmXZcESLSP3nnvdjt619l4bTPjNwWhccqIfgp7zW1BEI6LLfTqLon\n",
        "7fwFLDFmdni5ko/NvOUhjabQUNnwgfp2T+mUFYtEwWGFOItuha55wlUi5UG7ZVrG\n",
        "GnrVEWV4JReXAr83fMWKGiPToy92GZgtkUkM1rfGy5qePNIMvy903u2cnwHNU2lm\n",
        "WfFNJ04uykQrI+CVo1kPi5mbJlYe/VjrawKCAQEA5Pmjb8/MdAUEkb3zAD7GJIKC\n",
        "HnUAA4mwk8xVdsGN6xvUL8RYgi+VjSKvzNsUln5sPXdtZbP//gQOF7KgLPFFe+mf\n",
        "Xok7fGSTQ1DgVWEErFynAYxu+Uu4xtjRbPyCXjyoHianXkn3QDf1ggpF+y2R0Ivu\n",
        "oyxsDvMArFalbmK4q/+Q6/z/DtnirfjUnxiYEPEBZtP3Gz74KQK/AhForVlCiSz6\n",
        "MbDp30cxPy/8/pimJ9xUR6re9Xuw/EFWp0ifHXv6IGNOd8UQGejyI82KnJZPNTde\n",
        "tHO70d3zFdhrpJO63Elrw6c9bxeZrcJTT1e3wFpX2z1aE4dybdNqrI/IbzcdVA==\n",
        "-----END RSA PRIVATE KEY-----\n",
        "\"\"\")\n",
        "! ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "! chmod go-rwx /root/.ssh/id_rsa\n",
        "! git clone git@github.com:Tien-Cheng/dele-generative-adversarial-networks.git\n",
        "%cd /content/dele-generative-adversarial-networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRBkXblK-YpQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U torch-fidelity wandb torch-summary pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w4OzpB6-c_1"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw-0sgET-fKb"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zumKbcFW-cqB",
        "outputId": "c017a963-122d-4adf-de37-4f9bbb186895"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from collections import OrderedDict\n",
        "from typing import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch_fidelity\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, ModelSummary\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import *\n",
        "\n",
        "from torchvision import transforms as T\n",
        "\n",
        "import wandb\n",
        "from data.dataset import CIFAR10DataModule\n",
        "from utils.DiffAugment_pytorch import (\n",
        "    DiffAugment,\n",
        ")  # Make use of the official implementation of DiffAugment\n",
        "from utils.ema import EMA\n",
        "from utils.layers import (\n",
        "    ConditionalBatchNorm2d,\n",
        "    NormalizeInverse,\n",
        "    ResidualBlockDiscriminator,\n",
        "    ResidualBlockDiscriminatorHead,\n",
        "    ResidualBlockGenerator,\n",
        ")\n",
        "from utils.visualize import visualize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0RQBSYpS_HQ"
      },
      "source": [
        "### Basic Hyperparameters\n",
        "\n",
        "The most important hyperparameter for the data loader is the batch size, which defines how many real images (and thus how many generated fake images) will be fed into the discriminator during training of either the generator or discriminator.\n",
        "\n",
        "It was found in Brock, Donahue, and Simonyan, â€œLarge Scale GAN Training for High Fidelity Natural Image Synthesis.â€ that larger batch sizes could improve the overall quality of the GAN output, but would lead to more unstable training.\n",
        "\n",
        "> We begin by increasing the batch size for the baseline model, and immediately find tremendous benefits in doing so. Rows 1-4 of Table 1 show that simply increasing the batch size by a factor of 8 improves the state-of-the-art IS by 46%. We conjecture that this is a result of each batch covering more modes, providing better gradients for both networks. One notable side effect of this scaling is that our models reach better final performance in fewer iterations, but become unstable and undergo complete training collapse.\n",
        "\n",
        "As a result of their findings, I will try and use as large a batch size as possible, within the limitations of the GPU memory. To increase my batch size further, I can make use of gradient accumulation, which is helpfully implemented in PyTorch Lightning's Trainers, to accumulate the gradients from backprogagation over multiple batches, resulting in an increased effective batch size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gLPRMOsgSW8w",
        "outputId": "cae5707b-9a4a-4e95-f7fa-397a39c719f6"
      },
      "outputs": [],
      "source": [
        "# @title Basic Hyperparameters { run: \"auto\" }\n",
        "DATA_DIR = \"./data\"  # @param {type:\"string\"}\n",
        "BATCH_SIZE = 256  # @param {type:\"integer\"}\n",
        "NUM_WORKERS = 6  # @param {type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data\n",
        "\n",
        "## CIFAR-10\n",
        "\n",
        "CIFAR-10 is a labelled subset of the 80 million tiny images dataset. It consists of 60000 32x32 color images in 10 classes.\n",
        "\n",
        "- They are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "\n",
        "There are 6000 images per class. CIFAR-10 splits data into 50000 training images, and 10000 test images. It is a common benchmark dataset, used to evaluate computer vision models. It is also commonly used as a benchmark for GAN training, as GAN training typically takes a long time, and so a smaller dataset like CIFAR with a lower resolution is easier to train, allowing GAN models to be more easily evaluated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu9T96d-_Rnf"
      },
      "source": [
        "## Data Ingestion and Preprocessing\n",
        "\n",
        "To make the set up of the dataset easier and consistent for multiple experiments, a PyTorch Lightning (PL) Data Module is defined as follows. A `LightningDataModule` is a custom class in PL, which defines a structure for defining a dataset, such that it can be easily loaded by a PL Trainer.\n",
        "\n",
        "### Preprocessing\n",
        "\n",
        "A common preprocessing step when training a GAN is to normalize the dataset images such that they are within the range of -1 to 1. By performing feature scaling, I help the model to converge faster by gradient descent.\n",
        "\n",
        "### Augmentation\n",
        "\n",
        "Data augmentation is common applied in deep learning as a technique for preventing overfitting of a model by randomly applying transformations to images to generate variations of it. In this case, we want to reduce overfitting in the discriminator, as an overfitting discriminator, as found in Karras et al., â€œTraining Generative Adversarial Networks with Limited Data.â€ can lead to reduce quality of the generated images.\n",
        "\n",
        "> The distributions overlap initially but keep drifting apart as the discriminator becomes more and more confident, and the point where FID starts to deteriorate is consistent with the loss of sufficient overlap between distributions. This is a strong indication of overfitting, evidenced further by the drop in accuracy measured for a separate validation set. - Section 2, Overfitting in GANs\n",
        "\n",
        "When performing data augmentation in GAN training, there are some things to take note of:\n",
        "\n",
        "- Some augmentations are \"leaky\", meaning that the generator may learn to generate augmented images as it is unable to separate the augmentation from the data distribution.\n",
        "\n",
        "As such, as a start, I will make use of basic random horizontal flips, as they are quite safe (even if the augmentation leaks, there is nothing unrealistic about the image in the context of CIFAR10), and it is not a very strong augmentation, so it should not overpower the discriminator early on in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Q4fq4ltATFhN",
        "outputId": "a6183af8-1173-4ab6-8f84-884ed1dfbf5f"
      },
      "outputs": [],
      "source": [
        "preprocessing = [T.ToTensor(), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "basic_aug = [T.RandomHorizontalFlip(p=0.3)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "P1YteEfJSKQY",
        "outputId": "f6c13f91-f7e7-4237-de35-de50e6c4c2fc"
      },
      "outputs": [],
      "source": [
        "dm = CIFAR10DataModule(\n",
        "    data_dir=DATA_DIR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    transforms=preprocessing,\n",
        "    augments=basic_aug,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag08rZ3W_d3E"
      },
      "source": [
        "# Building a Conditional DCGAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf2aymldARgo"
      },
      "source": [
        "## Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iMQ65GSH_3sf"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_dim: int = 128,\n",
        "        embed_dim: int = 256,\n",
        "        num_filters: int = 64,\n",
        "        num_classes: int = 10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_filters = num_filters  # Base Number of Filters in Generator Blocks\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.label_embedding = nn.Embedding(num_classes, embed_dim)\n",
        "        self.latent = nn.Linear(latent_dim + embed_dim, latent_dim)\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, num_filters * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(num_filters * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(num_filters * 4, num_filters * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_filters * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(num_filters * 2, num_filters, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(num_filters, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: list = None):\n",
        "        y_embed = self.label_embedding(y)\n",
        "        y_embed /= torch.norm(y_embed, p=2, dim=1, keepdim=True)\n",
        "        conditional_inputs = torch.cat([x, y_embed], dim=1)\n",
        "        conditional_inputs = self.latent(conditional_inputs)\n",
        "        conditional_inputs = conditional_inputs.view(\n",
        "            conditional_inputs.shape[0], self.latent_dim, 1, 1\n",
        "        )\n",
        "        fake = self.main(conditional_inputs)\n",
        "        if not self.training:\n",
        "            fake = 255 * (fake.clamp(-1, 1) * 0.5 + 0.5)\n",
        "            fake = fake.to(torch.uint8)\n",
        "        return fake\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCRwH4cAATql"
      },
      "source": [
        "## Discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pj4rGgeiAVkX"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_filters: int = 64, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.num_filters = num_filters\n",
        "        self.num_classes = num_classes\n",
        "        self.label_embedding = nn.Embedding(num_classes, 32 * 32)\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(4, num_filters, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(num_filters, num_filters * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_filters * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(num_filters * 2, num_filters * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_filters * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.Conv2d(\n",
        "                num_filters * 4, 1, 4, 1, 0, bias=False\n",
        "            ),  # Apply a 4x4 Convolution to a 4x4 input, resulting in a 1x1 output\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: list = None):\n",
        "        labels = self.label_embedding(y)\n",
        "        labels /= torch.norm(labels, p=2, dim=1, keepdim=True)\n",
        "        labels = labels.view(labels.shape[0], 1, 32, 32)\n",
        "        conditional_inputs = torch.cat([x, labels], dim=1)  # Concat as Extra Channel\n",
        "        return self.main(conditional_inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRpJbKQRBDTY"
      },
      "source": [
        "## Conditional DCGAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C00_pmG1ENMo"
      },
      "outputs": [],
      "source": [
        "class ConditionalDCGAN(LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_dim: int = 128,\n",
        "        embed_dim: int = 128,\n",
        "        num_classes: int = 10,\n",
        "        g_lr: float = 0.0002,\n",
        "        d_lr: float = 0.0002,\n",
        "        adam_betas: Tuple[float, float] = (0.0, 0.999),\n",
        "        batch_size: int = 64,\n",
        "        d_steps: int = 1,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.g_lr = g_lr\n",
        "        self.d_lr = d_lr\n",
        "        self.betas = adam_betas\n",
        "        self.batch_size = batch_size\n",
        "        self.d_steps = d_steps  # Number of Discriminator steps per Generator Step\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.G = Generator(\n",
        "            latent_dim=self.latent_dim,\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_classes=num_classes,\n",
        "        )\n",
        "\n",
        "        self.D = Discriminator(num_classes=num_classes)\n",
        "\n",
        "        self.G.apply(self._weights_init)\n",
        "        self.D.apply(self._weights_init)\n",
        "\n",
        "        self.adversarial_loss = nn.BCELoss()  # BCE Loss\n",
        "\n",
        "        self.viz_z = torch.randn(64, self.latent_dim)\n",
        "        self.viz_labels = torch.LongTensor(torch.randint(0, self.num_classes, (64,)))\n",
        "\n",
        "        self.unnormalize = NormalizeInverse((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "    @staticmethod\n",
        "    def _weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find(\"Conv\") != -1:\n",
        "            torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        elif classname.find(\"BatchNorm\") != -1:\n",
        "            torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        return self.G(z, labels)\n",
        "\n",
        "    # Alternating schedule for optimizer steps (i.e.: GANs)\n",
        "    def optimizer_step(\n",
        "        self,\n",
        "        epoch,\n",
        "        batch_idx,\n",
        "        optimizer,\n",
        "        optimizer_idx,\n",
        "        optimizer_closure,\n",
        "        on_tpu,\n",
        "        using_native_amp,\n",
        "        using_lbfgs,\n",
        "    ):\n",
        "        # update discriminator opt every step\n",
        "        if optimizer_idx == 1:\n",
        "            optimizer.step(closure=optimizer_closure)\n",
        "\n",
        "        # update generator opt every 4 steps\n",
        "        if optimizer_idx == 0:\n",
        "            if (batch_idx + 1) % self.d_steps == 0:\n",
        "                optimizer.step(closure=optimizer_closure)\n",
        "            else:\n",
        "                # call the closure by itself to run `training_step` + `backward` without an optimizer step\n",
        "                optimizer_closure()\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        imgs, labels = batch  # Get real images and corresponding labels\n",
        "\n",
        "        # Generate Noise Vector z\n",
        "        z = torch.randn(imgs.shape[0], self.latent_dim)\n",
        "        z = z.type_as(imgs)  # Ensure z runs on the same device as the images\n",
        "        self.fake_labels = torch.LongTensor(\n",
        "            torch.randint(0, self.num_classes, (imgs.shape[0],))\n",
        "        ).to(self.device)\n",
        "        # Train Generator\n",
        "        if optimizer_idx == 0:\n",
        "            # Generate Images\n",
        "            self.fakes = self.forward(z, self.fake_labels)\n",
        "\n",
        "            # Classify Generated Images with Discriminator\n",
        "            fake_preds = torch.squeeze(self.D(self.fakes, self.fake_labels))\n",
        "\n",
        "            # We want to penalize the Generator if the Discriminator predicts it as fake\n",
        "            # Hence, set the target as a 1's vector\n",
        "            target = torch.ones(imgs.shape[0]).type_as(imgs)\n",
        "\n",
        "            g_loss = self.adversarial_loss(fake_preds, target)\n",
        "\n",
        "            self.log(\n",
        "                \"train_gen_loss\",\n",
        "                g_loss,\n",
        "                on_epoch=True,\n",
        "                on_step=False,\n",
        "                prog_bar=True,\n",
        "            )  # Log Generator Loss\n",
        "            tqdm_dict = {\n",
        "                \"g_loss\": g_loss,\n",
        "            }\n",
        "            output = OrderedDict(\n",
        "                {\"loss\": g_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
        "            )\n",
        "            return output\n",
        "\n",
        "        # Train Discriminator\n",
        "        if optimizer_idx == 1:\n",
        "            # Train on Real Data\n",
        "            real_preds = torch.squeeze(self.D(imgs, labels))\n",
        "            target = torch.ones(imgs.shape[0]).type_as(imgs)\n",
        "            d_real_loss = self.adversarial_loss(real_preds, target)\n",
        "\n",
        "            # Train on Generated Images\n",
        "            self.fakes = self.forward(z, self.fake_labels)\n",
        "            target = torch.zeros(imgs.shape[0]).type_as(imgs)\n",
        "            fake_preds = torch.squeeze(self.D(self.fakes, self.fake_labels))\n",
        "            d_fake_loss = self.adversarial_loss(fake_preds, target)\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "            self.log(\n",
        "                \"train_discriminator_loss\",\n",
        "                d_loss,\n",
        "                on_epoch=True,\n",
        "                on_step=False,\n",
        "                prog_bar=True,\n",
        "            )\n",
        "            tqdm_dict = {\n",
        "                \"d_loss\": d_loss,\n",
        "            }\n",
        "            output = OrderedDict(\n",
        "                {\"loss\": d_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
        "            )\n",
        "            return output\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        # Log Sampled Images\n",
        "        sample_imgs = self.unnormalize(self.fakes[:64]).cpu().detach()\n",
        "        sample_labels = self.fake_labels[:64].cpu().detach()\n",
        "        num_rows = int(np.floor(np.sqrt(len(sample_imgs))))\n",
        "        fig = visualize(sample_imgs, sample_labels, grid_shape=(num_rows, num_rows))\n",
        "        self.logger.log_image(key=\"generated_images\", images=[fig])\n",
        "        plt.close(fig)\n",
        "        del sample_imgs\n",
        "        del sample_labels\n",
        "        del fig\n",
        "        gc.collect()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        pass\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        sample_imgs = self.forward(\n",
        "            self.viz_z.to(self.device), self.viz_labels.to(self.device)\n",
        "        ).cpu().detach()\n",
        "        fig = visualize(sample_imgs, self.viz_labels.cpu().detach(), grid_shape=(8, 8))\n",
        "        metrics = torch_fidelity.calculate_metrics(\n",
        "            input1=torch_fidelity.GenerativeModelModuleWrapper(\n",
        "                self.G, self.latent_dim, \"normal\", self.num_classes\n",
        "            ),\n",
        "            input1_model_num_samples=50000,\n",
        "            input2=\"cifar10-train\",\n",
        "            isc=True,\n",
        "            fid=True,\n",
        "            kid=True,\n",
        "        )\n",
        "        self.logger.log_image(key=\"Validation Images\", images=[fig])\n",
        "        plt.close(fig)\n",
        "        del sample_imgs\n",
        "        del fig\n",
        "        gc.collect()\n",
        "        self.log(\"FID\", metrics[\"frechet_inception_distance\"], prog_bar=True)\n",
        "        self.log(\"IS\", metrics[\"inception_score_mean\"], prog_bar=True)\n",
        "        self.log(\"KID\", metrics[\"kernel_inception_distance_mean\"], prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Define the optimizers and schedulers for PyTorch Lightning\n",
        "\n",
        "        :return: A tuple of two lists - a list of optimizers and a list of learning rate schedulers\n",
        "        :rtype: Tuple[List, List]\n",
        "        \"\"\"\n",
        "        opt_G = Adam(\n",
        "            self.G.parameters(), lr=self.g_lr, betas=self.betas\n",
        "        )  # optimizer_idx = 0\n",
        "        opt_D = Adam(\n",
        "            self.D.parameters(), lr=self.d_lr, betas=self.betas\n",
        "        )  # optimizer_idx = 1\n",
        "        return [opt_G, opt_D], []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9LXzAqIAbY0"
      },
      "source": [
        "## Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtiencheng\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_everything(42)\n",
        "wandb_logger = WandbLogger(project=\"DELE_CA2_GAN\", log_model=\"all\")\n",
        "model = ConditionalDCGAN(g_lr=0.0002, d_lr=0.0002, batch_size=BATCH_SIZE)\n",
        "trainer = Trainer(\n",
        "    check_val_every_n_epoch=5,\n",
        "    logger=wandb_logger,\n",
        "    max_epochs=1000,\n",
        "    callbacks=[\n",
        "        ModelSummary(3),\n",
        "    ],\n",
        "    gpus=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.fit(model, dm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOI56mbxgkF+EXTRTq++yJk",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "DELE CA2 Part I - Controllable & Conditional GANS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
